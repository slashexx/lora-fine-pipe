{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb3bfbf2",
   "metadata": {},
   "source": [
    "# üöÄ Automated LoRA Training Pipeline\n",
    "\n",
    "This notebook automatically trains a LoRA adapter for your selected model using your custom dataset.\n",
    "\n",
    "**Steps:**\n",
    "1. Install required packages\n",
    "2. Load your dataset from the backend\n",
    "3. Configure LoRA parameters\n",
    "4. Train the model\n",
    "5. Save and upload the trained adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f71323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers datasets peft accelerate bitsandbytes trl einops requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69752f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Parse URL parameters\n",
    "import sys\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "\n",
    "# Get job_id and model from URL (will be passed as parameters)\n",
    "# For manual testing, set these values:\n",
    "JOB_ID = \"test-job-123\"  # Will be replaced by actual job ID from URL\n",
    "MODEL_NAME = \"llama-2-7b\"  # Will be replaced by selected model\n",
    "API_URL = \"https://slmllm-backend.vercel.app\"\n",
    "\n",
    "print(f\"Job ID: {JOB_ID}\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"API URL: {API_URL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cde201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model mapping\n",
    "MODEL_MAP = {\n",
    "    \"llama-2-7b\": \"meta-llama/Llama-2-7b-hf\",\n",
    "    \"llama-2-13b\": \"meta-llama/Llama-2-13b-hf\",\n",
    "    \"mistral-7b\": \"mistralai/Mistral-7B-v0.1\",\n",
    "    \"phi-2\": \"microsoft/phi-2\",\n",
    "    \"gemma-7b\": \"google/gemma-7b\",\n",
    "    \"tinyllama-1.1b\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "}\n",
    "\n",
    "model_id = MODEL_MAP.get(MODEL_NAME, \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "print(f\"Using model: {model_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3e0468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to update training status\n",
    "def update_status(status, progress=0, error=None):\n",
    "    try:\n",
    "        data = {\n",
    "            \"status\": status,\n",
    "            \"progress\": progress\n",
    "        }\n",
    "        if error:\n",
    "            data[\"error\"] = error\n",
    "        \n",
    "        response = requests.post(\n",
    "            f\"{API_URL}/api/train/update/{JOB_ID}\",\n",
    "            data=data\n",
    "        )\n",
    "        print(f\"Status update: {status} ({progress}%)\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to update status: {e}\")\n",
    "\n",
    "# Update status to training\n",
    "update_status(\"training\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1c3563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset (simulated - in production, fetch from backend)\n",
    "# For now, create a sample dataset\n",
    "sample_data = {\n",
    "    \"input\": [\n",
    "        \"What is machine learning?\",\n",
    "        \"Explain neural networks\",\n",
    "        \"What is deep learning?\"\n",
    "    ],\n",
    "    \"output\": [\n",
    "        \"Machine learning is a subset of AI that enables systems to learn from data.\",\n",
    "        \"Neural networks are computational models inspired by the human brain.\",\n",
    "        \"Deep learning is a subset of machine learning using multi-layered neural networks.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(sample_data)\n",
    "print(f\"Loaded {len(df)} training examples\")\n",
    "print(df.head())\n",
    "\n",
    "update_status(\"training\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c802bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import training libraries\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer\n",
    "from datasets import Dataset\n",
    "\n",
    "update_status(\"training\", 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f048974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure 4-bit quantization for efficient training\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load model and tokenizer\n",
    "print(f\"Loading {model_id}...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "update_status(\"training\", 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d2ac98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # Rank\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Prepare model for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable params: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)\")\n",
    "\n",
    "update_status(\"training\", 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a63e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset for training\n",
    "def format_instruction(sample):\n",
    "    return f\"### Input:\\n{sample['input']}\\n\\n### Output:\\n{sample['output']}\"\n",
    "\n",
    "# Convert to HuggingFace dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "dataset = dataset.map(lambda x: {\"text\": format_instruction(x)})\n",
    "\n",
    "print(f\"Dataset prepared with {len(dataset)} examples\")\n",
    "update_status(\"training\", 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223f030e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora_output\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=10,\n",
    "    save_steps=50,\n",
    "    warmup_steps=10,\n",
    "    max_grad_norm=0.3,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=512,\n",
    ")\n",
    "\n",
    "print(\"Trainer configured successfully!\")\n",
    "update_status(\"training\", 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333a7cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"üöÄ Starting training...\")\n",
    "update_status(\"training\", 45)\n",
    "\n",
    "try:\n",
    "    trainer.train()\n",
    "    print(\"‚úÖ Training completed successfully!\")\n",
    "    update_status(\"training\", 90)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Training failed: {e}\")\n",
    "    update_status(\"failed\", 0, str(e))\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72796e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "output_dir = f\"./lora_model_{JOB_ID}\"\n",
    "trainer.model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"Model saved to {output_dir}\")\n",
    "update_status(\"training\", 95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4267fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trained model\n",
    "print(\"\\nüß™ Testing trained model...\")\n",
    "test_prompt = \"What is machine learning?\"\n",
    "inputs = tokenizer(format_instruction({\"input\": test_prompt, \"output\": \"\"}), return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"\\nPrompt: {test_prompt}\")\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b64925e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the training job\n",
    "update_status(\"completed\", 100)\n",
    "print(\"\\n‚úÖ Training pipeline completed successfully!\")\n",
    "print(f\"\\nüì¶ Your trained LoRA adapter is ready in: {output_dir}\")\n",
    "print(\"\\nüí° To use this model:\")\n",
    "print(\"1. Download the adapter files from this Colab\")\n",
    "print(\"2. Load it with PEFT in your application\")\n",
    "print(\"3. Merge with base model or use directly\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
